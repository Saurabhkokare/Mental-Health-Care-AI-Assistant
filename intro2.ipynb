{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saurabh_kokare/Music/llama_project/venv/lib/python3.12/site-packages/gradio/components/chatbot.py:248: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saurabh_kokare/Music/llama_project/venv/lib/python3.12/site-packages/gradio/blocks.py:1780: UserWarning: A function (_submit_fn) returned too many output values (needed: 2, returned: 3). Ignoring extra values.\n",
      "    Output components:\n",
      "        [state, chatbot]\n",
      "    Output values returned:\n",
      "        [\"\", [('will you give me some tips regarding depression', 'To help with depression, consider the following tips: \\nOffer support and emphasize that there is no shame in feeling depressed. \\nHelp the individual identify others who can serve as a support, such as family and friends. \\nHelp them identify and focus on personal strengths and the positives in a challenging situation. \\nCreating a supportive environment and encouraging self-care can also be beneficial. \\nAdditionally, counseling and education about symptoms, monitoring for suicidal thoughts, and in severe cases, medication can be helpful.'), ('will you give me some tips regarding depression', '')], [('will you give me some tips regarding depression', 'To help with depression, consider the following tips: \\nOffer support and emphasize that there is no shame in feeling depressed. \\nHelp the individual identify others who can serve as a support, such as family and friends. \\nHelp them identify and focus on personal strengths and the positives in a challenging situation. \\nCreating a supportive environment and encouraging self-care can also be beneficial. \\nAdditionally, counseling and education about symptoms, monitoring for suicidal thoughts, and in severe cases, medication can be helpful.')]]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.environ.get(\"GROQ_API_KEY\") \n",
    "\n",
    "\n",
    "# Initialize the LLM\n",
    "def init_llm():\n",
    "    \"\"\"Initialize the Groq language model.\"\"\"\n",
    "    llm = ChatGroq(\n",
    "        temperature=0,\n",
    "        groq_api_key=groq_api_key,\n",
    "        model=\"llama-3.3-70b-versatile\"\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "\n",
    "# Load PDF data and embed into ChromaDB\n",
    "def load_data(data_path):\n",
    "    \"\"\"Load PDF data, split text, embed, and store in Chroma DB.\"\"\"\n",
    "    loader = DirectoryLoader(data_path, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v1\")\n",
    "    vector_db = Chroma.from_documents(texts, embeddings, persist_directory=\"./chroma_db\")\n",
    "    vector_db.persist()\n",
    "    \n",
    "    return vector_db\n",
    "\n",
    "\n",
    "# Set up QA Chain\n",
    "def setup_QA_chain(vector_db, llm):\n",
    "    \"\"\"Set up the RetrievalQA chain.\"\"\"\n",
    "    retriever = vector_db.as_retriever()\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "    Use the following pieces of information to answer the user's question.Don't give page number to users give answer to user from that page. \n",
    "    If you don't know the answer, just say 'I don't know', don't try to make up an answer.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Only return the helpful answer below and nothing else.\n",
    "    Helpful answer:\n",
    "    \"\"\"\n",
    "    PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    \n",
    "    QA_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT}\n",
    "    )\n",
    "    return QA_chain\n",
    "\n",
    "\n",
    "# Chatbot response for Gradio\n",
    "def chatbot_response(user_input, history=[]):\n",
    "    \"\"\"Handle user queries via Gradio ChatInterface.\"\"\"\n",
    "    if not user_input.strip():\n",
    "        return \"Please provide a valid input.\", history\n",
    "    \n",
    "    try:\n",
    "        response = qa_chain.run(user_input)\n",
    "        history.append((user_input, response))\n",
    "        return \"\",history\n",
    "    except Exception as e:\n",
    "        history.append((user_input, f\"An error occurred: {e}\"))\n",
    "        return history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    global qa_chain  \n",
    "    \n",
    "    llm = init_llm()\n",
    "    db_path = \"./chroma_db\"\n",
    "    data_path = \"./data\"  # Adjust this path to your PDF folder\n",
    "    \n",
    "    if not os.path.exists(db_path):\n",
    "        print(\"Database not found. Creating a new vector database...\")\n",
    "        vector_db = load_data(data_path)\n",
    "    else:\n",
    "        print(\"Loading existing vector database...\")\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v1\")\n",
    "        vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)\n",
    "    \n",
    "    qa_chain = setup_QA_chain(vector_db, llm)\n",
    "    \n",
    "    with gr.Blocks(theme='Respair/Shiki@1.2.1') as app:\n",
    "        gr.Markdown(\n",
    "        \"<h1 style='text-align: center; color: #F9D223FF;'>Mental Health Care AI Assistant</h1>\",\n",
    "        elem_id=\"title\"\n",
    "    )\n",
    "        chatbot=gr.ChatInterface(\n",
    "            fn=chatbot_response,\n",
    "            #title=\"Mental Health AI Assistant\",\n",
    "            fill_height=1200,\n",
    "            description=\"Ask questions regarding Mental Health Care.\",\n",
    "        )\n",
    "\n",
    "    \n",
    "    app.launch()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio_client in ./venv/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from gradio_client) (2024.12.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in ./venv/lib/python3.12/site-packages (from gradio_client) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in ./venv/lib/python3.12/site-packages (from gradio_client) (0.27.0)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from gradio_client) (24.2)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in ./venv/lib/python3.12/site-packages (from gradio_client) (4.12.2)\n",
      "Requirement already satisfied: websockets<15.0,>=10.0 in ./venv/lib/python3.12/site-packages (from gradio_client) (14.1)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.12/site-packages (from httpx>=0.24.1->gradio_client) (4.7.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.12/site-packages (from httpx>=0.24.1->gradio_client) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx>=0.24.1->gradio_client) (1.0.7)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.12/site-packages (from httpx>=0.24.1->gradio_client) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio_client) (0.14.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->gradio_client) (3.16.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->gradio_client) (6.0.2)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->gradio_client) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./venv/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->gradio_client) (4.67.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.12/site-packages (from anyio->httpx>=0.24.1->gradio_client) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->gradio_client) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->gradio_client) (2.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install gradio_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
